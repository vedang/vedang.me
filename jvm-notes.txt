-*- mode: org; comment-column: 0; -*-

# This file contains a list of java/jvm commands that I've found handy
# in situations I've faced over time. These are commands that _I_ have
# used (successfully).  I'm not claiming that this is the only way or
# even the right way to handle a particular situation that you may
# face. YMMV.


* Basics

** Print Java processes running on the system

$ jps

** Take thread-dumps of a given process

$ sudo jstack -F <pid> > /tmp/threaddump

** Take heap dumps of a given process

$ jmap -dump:format=b,file=cheap.bin <pid>

* Multiple JDKs
  *Note*: The following function is taken from a stack overflow
  answer:
  - If you are moving to Java 8 and still want to run Java 7 the
    following script might help you. Insert the below code block in
    your ~.bashrc~ or ~.bash_profile~ scripts in your home directory:
    #+begin_src shell
      function setjdk() {
        if [ $# -ne 0 ]; then
         removeFromPath '/System/Library/Frameworks/JavaVM.framework/Home/bin'
         if [ -n "${JAVA_HOME+x}" ]; then
          removeFromPath $JAVA_HOME
         fi
         export JAVA_HOME=`/usr/libexec/java_home -v $@`
         export PATH=$JAVA_HOME/bin:$PATH
        fi
       }

      function removeFromPath() {
        export PATH=$(echo $PATH | sed -E -e "s;:$1;;" -e "s;$1:?;;")
      }
    #+end_src
  - Use ~setjdk~ to switch between versions on the command line as follows:
    #+begin_example
      $ setjdk 1.7
      $ java -version
      java version "1.7.0_60"
      Java(TM) SE Runtime Environment (build 1.7.0_60-b19)
      Java HotSpot(TM) 64-Bit Server VM (build 24.60-b09, mixed mode)

      $ setjdk 1.8
      java version "1.8.0_66"
      Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
      Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
    #+end_example
* Debugging

** Print the values of all the flags for the JVM.

# This will print the default values of all flags.
$ java -XX:+AggressiveOpts -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal

# Choose your flags and run the command again, check diff to see all
# the stuff that changes because of your change.
$ java -XX:+AggressiveOpts -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -XX:+UseConcMarkSweepGC

** Browsing JMX beans on a production server
$ j4psh http://localhost:8778/jolokia

* GC Tuning
** Thumb Rules of GC Tuning

1. Try to maximize objects reclaimed in young gen. Minimize Full GC
   frequency.
2. Set -Xms and -Xmx to 3x to 4x of LDS.
3. Set both -XX:PermSize and -XX:MaxPermSize to 1.5x max perm gen size.
4. Young gen should be 1.5x of LDS. (around 1/3 - 1/4 of heap size)
   (use -XX:NewRatio to set ratio of young gen to heap size)
5. Old gen should be 2x to 3x of LDS.
6. Which garbage collector should I use?
   Read: https://blogs.oracle.com/jonthecollector/entry/our_collectors

** Enable GC Logging in production for better insights.

   - Interesting Command Line flags ::
        -Xloggc:<file>
        -XX:+CMSIncrementalMode
        -XX:+DoEscapeAnalysis
        -XX:+GCLogFileSize=10M
        -XX:InitialHeapSize=3758096384
        -XX:MaxHeapSize=3758096384
        -XX:MaxTenuringThreshold=6
        -XX:NewRatio=4
        -XX:NumberOfGCLogFiles=10
        -XX:OldPLABSize=16
        -XX:+PrintCommandLineFlags
        -XX:+PrintGC
        -XX:+PrintGCDateStamps
        -XX:+PrintGCDetails
        -XX:+PrintGCTimeStamps
        -XX:+PrintTenuringDistribution
        -XX:+UseBiasedLocking
        -XX:+UseCompressedClassPointers
        -XX:+UseCompressedOops
        -XX:+UseConcMarkSweepGC
        -XX:+UseGCLogFileRotation
        -XX:+UseParNewGC
** Use [[https://github.com/chewiebug/GCViewer][GCViewer]] for offline analysis of GC logs.

** Calculate Live Data Size (LDS) to see which objects are being instantiated.

$ jmap -histo:live <pid>

# GC Log will also give approximation on LDS (Heap occupancy after
# each full GC), max perm gen size, and worst case latency scenario
# due to full GC
** Parallel Scavenge GC
- Default in JDK 6, stop-the-world GC
- -XX:+UseParallelGC (for new gen)
- -XX:+UseParallelOldGC (for old gen)
- By default, GC threads = number of cores.
  Control number using -XX:+ParallelGCThreads=<n>
** Concurrent Mark and Sweep GC
- Low latencies collector, very small stop-the-world (for initial
  marking)
- to improve throughput, use ParNewGC for young gen
  -XX:+UseParNewGC (and -XX:+ParallelGCThreads=<n>)
  (CMS will run on old gen and parnew on new gen, so there will be a
  small pause for new gen objects)
- CMS with ParNewGC is a good option for availability
** G1 GC (The Garbage First GC)
- officially supported from JDK 7u4
- -XX:+UseG1GC
- This is replacement for CMS.
  - parallel
  - concurrent
  - generational
  - good throughput
  - low latency
  ** Differences with CMS begin here **
  - compacting
  - predictable
  - improved ease-of-use
- G1GC is great for:
  - Large heaps with limited GC latencies (typically 6GB or larger)
  - Varied object allocation rate, and undesired long GC/compaction
    pauses (> 0.5s)
- More Reading:
  http://www.infoq.com/presentations/java-g1
  http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html

* Finding Memory Leaks
** Extensive Leaks
When looking at an OutOfMemoryError induced heapdump post mortem, one can usually assume that the Memory Leak filled the majority of the memory. But how big is the leak? While the dump contains all object instances, Eclipse MAT shows in its Class Histogramm only class names, amount of instances and the sum of their shallow size. Shallow Size is basically the sum of the sizes of all contained primitive data types, as well as the size of the references to other objects. But the so called Retained Heap is more important for finding leaks, as it is the total size of all objects being kept alive by this dominator. To get this number, MAT needs to calculate the Retained Size by following all object references. As sorting by Retained Heap is very helpful for finding leaks, MAT displays the largest retained heaps in a pie chart. A prominent example for a lot of Retained Heap in web applications is usually the

org.apache.catalina.session.StandardManager

The purpose is clear: keep user data for the time of their session. But very often frameworks store a lot of data here as well. We already blogged about Ajax4JSF consuming a lot memory in the session. But also custom written, and unfortunately often incorrect, caches appear very often in the pie charts. This makes it very easy to locate memory leaks and their cause quickly.

To determine who is creating these objects, or find out what the purpose of some structures is, the actual instances with their incoming and outgoing references are required. To get them, choose List Objects with incoming References from the context menu. Now a tree structure is displayed, showing all instances with all incoming references. Those references keep the object alive and prevented them from being garbage collected. Outgoing References are interesting as well, because they show the actual contents of the instances, helping to find out their purpose. They are especially interesting when the actual dominator, like the SessionManagers, should not be removed, but the excessive objects referenced by it should.

** Sneaking Memory Leaks
Finding Memory Leaks long before an OutOfMemoryError occurs is possible by creating multiple snapshots with the previously described method. A single snapshot alone can seldom help deciding whether the amount of objects is ok or not. But slowly growing object counts can hint at leaks. Eclipse MAT allows comparing two heapdump snapshots and finding growing structures and instance counts. But this requires two well timed dumps. With some amount of load they should be minutes apart, with few load hours or days. Having a greater time lag helps in separating normal jitter from real issues.

** Tips and Tricks
 The following list of tips and tricks are very useful for proper memory dump analysis to go smoothly, so I highly recommend you use them.

    Reduce the size of your applicationÕs heap as much as possible before generating memory dumps. Analyzing heap memory dumps is a process that requires a lot of memory, and usually, despite the tools best efforts to reduce memory consumption, you will need at least as much free memory as the size of the uncompressed heap memory dump file. So for a 4GB dump file you will need 4GB of free memory on the machine that will perform the analysis. So making the JVM heap as small as possible will generate smaller heap dumps, which will therefore be easier to analyze. Of course this is not always possible but when it is this will prove extremely useful.

    Close as many applications as possible on the machine that will analyze the memory dump. As in the first point we illustrated that the analysis tools require lots of free memory, it is usually a good idea to temporarily dedicate the machine to the analysis tool, by closing as many running applications (and daemons) as possible. Usually I use my OSÕs task manager to see which applications are using up memory, and close those first. Unfortunately this also usually involves closing the Java IDE, which is usually an application that consumes a lot of memory. So, when possible, try to use another machine to look at the code at the same time.

    Usually you will want to use some kind of class histogram view, which lists the memory consumption of objects by class type. This makes it easier to understand what class type is consuming memory, which in turn will help you identify the reason why so much memory is being used. Try to avoid analyzing low level classes such as Strings or even primitive types such as byte arrays. Instead, navigate up their references to find which objects contain them to see what class is actually using the memory.

    Know the difference between shallow and retained sizes. The shallow size is the actual memory consumed by the direct fields of the object instance, that is to say all the fields that do NOT reference other Java objects. Usually it is quite small and not that interesting, unless it contains huge primitive type arrays, so the retained size will be more interesting. The retained size includes the referenced Java objects, so it is much more expensive to calculate, and some tools might defer calculation in order to avoid calculating as it is quite CPU intensive. In the case of the Eclipse Memory Analyzer for example, it calculates estimates of the retained size, but will require the user to actually trigger the generation of precise retained sizes. In general it is a good idea to calculate at least part of the retained sizes, because they might be VERY different from the estimated ones.

    The Eclipse Memory Analyzer project has a very powerful feature called Ògroup by valueÓ, which makes it possible to build an object query and regroup the instances by a field value. This is useful in the case where you have a lot of instances that are containing a smaller set of possible values, and you can to see which values are being used the most. This has really helped me understand some complex memory dumps so I recommend you try it out.

    There is a way to analyze serialized data ! Some memory dumps might contain serialized data, for example data that was sent over the network into a buffer object and that was not yet deserialized. This is especially true for JGroups buffers. If your profile offers the possibility to export the value of the serialized data to a file (Eclipse Memory Analyzer has this feature in the ÒCopyÓ -> ÒSave Value to fileÓ to contextual menu option), you can then use the following tool to deserialize the data (in the case of Eclipse files, you will need to skip the first byte of the file, so using something like : java -jar jdeserialize-1.3.jar -skipfirstbyte 1 test.dump). Knowing that you can actually inspect serialized data can be a lifesaver, because sometimes you might just give up when seeing a serialized data buffer when you can actually drill down into it (although there is not yet a fancy UI to do that :)).

    Did you know that JVM 1.6+ memory dumps contain thread dumps ? In any case, make sure you have a look at the thread dump, since it might help you understand what the threads were doing at the time of the memory dump. In the case of an OutOfMemory exception you might even be able to understand the source of the problem using the combination of memory snapshot and thread stacks.

    Use temporary Amazon EC3 instances if you need more RAM to analyze memory dumps. If you absolutely need to analyze a large memory dump (8GB or more) and donÕt have access to hardware that contains enough physical memory, donÕt forget that you could simply use a temporary Amazon EC3 instance to run your favorite memory analysis tool on such an instance. In one case I started a Windows Amazon instance with 32GB of RAM just for an hour, installed the YourKit profiler on there and I instantly had a machine dedicated to memory dump analysis. All of this for less than a dollar :)

    By default the Eclipse Memory Analyzer tool does not run with a large maximum heap size (1GB), so make sure you extend it before using it to open large heaps. You can find the instructions on how to do this here

    Sometimes the largest objects are no longer ÒliveÓ. If the memory dump you are analyzing contains the activity of an application that generates a lot of new objects very quickly, it might be that at the time of the dump the JVM garbage collector might not yet have been able to remove all the objects from memory. So make sure that you have a look at the Òunreachable objectsÓ size. The rule of thumb is this: if the total amount of (live) objects size in your analysis tool is much smaller than the size of the memory dump, it is highly likely that you are dealing with a memory dump that contains a lot of unreachable objects. Some tools take them automatically into account (as in the case of Yourkit), but in some other cases (like in the Eclipse Memory Analyzer case), you need to activate processing of such objects.

    The object explorer (or any similar functionality that makes it possible to simply explore objects) might be more useful than you think. Sometimes looking at a few sample objects, even if they cannot be statistically significant if there are large amount of these, does help get a better picture of how the data is structured in memory. So donÕt be afraid, even on large object collections, to drill down into a few instances to see if everything looks alright (or not).

    Although it is tempting, avoid as much as possible using allocation recording. Most profilers offer a feature called Òrecord allocationsÓ. Most of the time this feature will slow down the JVM to a crawl, so unless you have exhausted all other ways of analyzing the contents of the memory and what code is generating the objects, I strongly recommend against using this. Personally I tried to use it alot at first, but now I rarely activate it at all. In general, any feature that has a heavy performance impact is usually not very useful, since it completely changes the behavior of the application.
* MAT
  - Note: if the analysis fails with "Java Heap Space" you are not starting MAT with enough Heap Mem. Give the following options:
    #+begin_src sh
      $ MemoryAnalyzer -vmargs -Xmx8g -XX: -UsedGCOverheadLimit
    #+end_src
